{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIyta-NncCeG"
      },
      "source": [
        "# PHASE ONE\n",
        "\n",
        "## This code uses my input_sefaria_verses.csv as baseline which we have already edited out vowels from"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "\n",
        "INPUT_VERSES_CSV = \"input_sefaria_verses.csv\"\n",
        "LXX_RETRO_CSV = \"lxx_hebrew_retroversions.csv\"\n",
        "OUTPUT_CSV = \"torah_reconstructed.csv\"\n",
        "\n",
        "BOOKS = [\"Genesis\", \"Exodus\", \"Leviticus\", \"Numbers\", \"Deuteronomy\"]\n",
        "\n",
        "def fetch_canonical_refs():\n",
        "    \"\"\"Build ordered list of Book,Chapter,Verse WITHOUT storing MT text\"\"\"\n",
        "    refs = []\n",
        "\n",
        "    for book in BOOKS:\n",
        "        chapter = 1\n",
        "        while True:\n",
        "            url = f\"https://www.sefaria.org/api/texts/{book}.{chapter}?lang=he\"\n",
        "            r = requests.get(url)\n",
        "\n",
        "            if r.status_code != 200:\n",
        "                break\n",
        "\n",
        "            data = r.json()\n",
        "            verses = data.get(\"he\")\n",
        "\n",
        "            if not verses:\n",
        "                break\n",
        "\n",
        "            for i in range(len(verses)):\n",
        "                refs.append((book, chapter, i + 1))\n",
        "\n",
        "            chapter += 1\n",
        "\n",
        "    return refs\n",
        "\n",
        "\n",
        "def load_user_verses():\n",
        "    verses = []\n",
        "    with open(INPUT_VERSES_CSV, encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            if row:\n",
        "                verses.append(row[0].strip())\n",
        "    return verses\n",
        "\n",
        "\n",
        "def load_retroversions():\n",
        "    retro = {}\n",
        "    with open(LXX_RETRO_CSV, encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for r in reader:\n",
        "            key = (r[\"Book\"], int(r[\"Chapter\"]), int(r[\"Verse\"]))\n",
        "            retro[key] = r[\"LXX_Retro_Hebrew\"]\n",
        "    return retro\n",
        "\n",
        "\n",
        "def reconstruct():\n",
        "    print(\"Loading canonical verse references...\")\n",
        "    refs = fetch_canonical_refs()\n",
        "\n",
        "    print(\"Loading user verses...\")\n",
        "    user_verses = load_user_verses()\n",
        "\n",
        "    print(\"Loading retroversions...\")\n",
        "    retro = load_retroversions()\n",
        "\n",
        "    if len(user_verses) != len(refs):\n",
        "        print(\"WARNING: verse count mismatch!\")\n",
        "        print(\"User:\", len(user_verses))\n",
        "        print(\"Canonical:\", len(refs))\n",
        "\n",
        "    rows = []\n",
        "    replacements = 0\n",
        "\n",
        "    for i in range(min(len(user_verses), len(refs))):\n",
        "        book, ch, vs = refs[i]\n",
        "        user_text = user_verses[i]\n",
        "\n",
        "        key = (book, ch, vs)\n",
        "\n",
        "        if key in retro:\n",
        "            final_text = retro[key]\n",
        "            replaced = \"YES\"\n",
        "            replacements += 1\n",
        "        else:\n",
        "            final_text = user_text\n",
        "            replaced = \"NO\"\n",
        "\n",
        "        rows.append([\n",
        "            book,\n",
        "            ch,\n",
        "            vs,\n",
        "            user_text,\n",
        "            final_text,\n",
        "            replaced\n",
        "        ])\n",
        "\n",
        "    print(\"Saving reconstructed Torah...\")\n",
        "\n",
        "    with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            \"Book\",\n",
        "            \"Chapter\",\n",
        "            \"Verse\",\n",
        "            \"Original_User_Text\",\n",
        "            \"Authoritative_Text\",\n",
        "            \"Replaced\"\n",
        "        ])\n",
        "        writer.writerows(rows)\n",
        "\n",
        "    print(\"DONE\")\n",
        "    print(\"Total replacements:\", replacements)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    reconstruct()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPppV5P-fEow",
        "outputId": "cdffd879-0ca4-4333-e923-1c00d69e8fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading canonical verse references...\n",
            "Loading user verses...\n",
            "Loading retroversions...\n",
            "Saving reconstructed Torah...\n",
            "DONE\n",
            "Total replacements: 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE TWO\n",
        "\n",
        "Indexing our result into a letter-table"
      ],
      "metadata": {
        "id": "xdjFOnqwhoM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import csv\n",
        "import re\n",
        "\n",
        "INPUT_FILE = \"torah_reconstructed.csv\"\n",
        "OUTPUT_FILE = \"torah_letter_index.csv\"\n",
        "\n",
        "\n",
        "def clean_word(word):\n",
        "    \"\"\"\n",
        "    Keep Hebrew letters only.\n",
        "    Removes punctuation, numbers, stray marks.\n",
        "    \"\"\"\n",
        "    return re.sub(r'[^א-ת]', '', word)\n",
        "\n",
        "\n",
        "def main():\n",
        "    index = 1\n",
        "    rows_out = []\n",
        "\n",
        "    with open(INPUT_FILE, newline='', encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for row in reader:\n",
        "            verse = row[\"Authoritative_Text\"].strip()\n",
        "\n",
        "            # Split into Hebrew words\n",
        "            words = verse.split()\n",
        "\n",
        "            for word in words:\n",
        "                clean = clean_word(word)\n",
        "\n",
        "                # Skip empty results\n",
        "                if not clean:\n",
        "                    continue\n",
        "\n",
        "                for letter in clean:\n",
        "                    rows_out.append({\n",
        "                        \"Index\": index,\n",
        "                        \"Letter\": letter,\n",
        "                        \"Hebrew Word\": clean\n",
        "                    })\n",
        "                    index += 1\n",
        "\n",
        "    # Write output\n",
        "    with open(OUTPUT_FILE, \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"Index\", \"Letter\", \"Hebrew Word\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows_out)\n",
        "\n",
        "    print(\"✅ Done.\")\n",
        "    print(\"Total letters indexed:\", index - 1)\n",
        "    print(\"Output saved to:\", OUTPUT_FILE)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9HXVVmThvp5",
        "outputId": "88297c69-9703-4628-ba1f-06d5bcbbd412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done.\n",
            "Total letters indexed: 304493\n",
            "Output saved to: torah_letter_index.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to count YHWH occurrences"
      ],
      "metadata": {
        "id": "WiUSS-hli7VD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is borrowed from paintedpotato (2024 project)\n",
        "\n",
        "NB: I counted 20 mentions of YHWH from the results"
      ],
      "metadata": {
        "id": "uN_1bHSGjrUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Function to read the CSV and load the letter data\n",
        "def read_torah_csv(filename):\n",
        "    letters_info = []\n",
        "\n",
        "    with open(filename, mode='r', encoding='utf-8') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for row in reader:\n",
        "            letters_info.append({\n",
        "                'Index': int(row['Index']),\n",
        "                'Letter': row['Letter'],\n",
        "                'Hebrew Word': row['Hebrew Word']\n",
        "            })\n",
        "\n",
        "    return letters_info\n",
        "\n",
        "# Function to find sequences of Tav, Vav, Resh, Hey separated by 50 letters\n",
        "def find_tav_vav_resh_hey_sequences(letters_info):\n",
        "    sequences = []\n",
        "\n",
        "    # Loop through the list of letters\n",
        "    for i in range(len(letters_info) - 150):\n",
        "        if (letters_info[i]['Letter'] == 'ת' and\n",
        "            letters_info[i + 50]['Letter'] == 'ו' and\n",
        "            letters_info[i + 100]['Letter'] == 'ר' and\n",
        "            letters_info[i + 150]['Letter'] == 'ה'):\n",
        "            sequences.append({\n",
        "                'Tav Index': letters_info[i]['Index'],\n",
        "                'Tav Word': letters_info[i]['Hebrew Word'],\n",
        "                'Vav Index': letters_info[i + 50]['Index'],\n",
        "                'Vav Word': letters_info[i + 50]['Hebrew Word'],\n",
        "                'Resh Index': letters_info[i + 100]['Index'],\n",
        "                'Resh Word': letters_info[i + 100]['Hebrew Word'],\n",
        "                'Hey Index': letters_info[i + 150]['Index'],\n",
        "                'Hey Word': letters_info[i + 150]['Hebrew Word']\n",
        "            })\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Function to print the sequences found\n",
        "def print_sequences(sequences):\n",
        "    for seq in sequences:\n",
        "        print(f\"Tav at index {seq['Tav Index']} (Word: {seq['Tav Word']})\")\n",
        "        print(f\"Vav at index {seq['Vav Index']} (Word: {seq['Vav Word']})\")\n",
        "        print(f\"Resh at index {seq['Resh Index']} (Word: {seq['Resh Word']})\")\n",
        "        print(f\"Hey at index {seq['Hey Index']} (Word: {seq['Hey Word']})\")\n",
        "        print('-' * 40)\n",
        "\n",
        "# Main process\n",
        "filename = 'torah_letter_index.csv'  # The CSV file generated earlier\n",
        "letters_info = read_torah_csv(filename)\n",
        "\n",
        "# Find and print the sequences\n",
        "sequences = find_tav_vav_resh_hey_sequences(letters_info)\n",
        "print_sequences(sequences)\n",
        "\n",
        "# Optionally, you can save the sequences to another CSV file if needed.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DWPwaoMRjAYg",
        "outputId": "b2e715f0-389a-44b5-a223-2aef49dc202a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tav at index 6 (Word: בראשית)\n",
            "Vav at index 56 (Word: תהום)\n",
            "Resh at index 106 (Word: וירא)\n",
            "Hey at index 156 (Word: אלהים)\n",
            "----------------------------------------\n",
            "Tav at index 18472 (Word: ותאמר)\n",
            "Vav at index 18522 (Word: והתעני)\n",
            "Resh at index 18572 (Word: מרב)\n",
            "Hey at index 18622 (Word: יהוה)\n",
            "----------------------------------------\n",
            "Tav at index 76069 (Word: וזאת)\n",
            "Vav at index 76119 (Word: ויאמר)\n",
            "Resh at index 76169 (Word: עפרון)\n",
            "Hey at index 76219 (Word: קנה)\n",
            "----------------------------------------\n",
            "Tav at index 94183 (Word: אתו)\n",
            "Vav at index 94233 (Word: בחפזון)\n",
            "Resh at index 94283 (Word: בארץ)\n",
            "Hey at index 94333 (Word: והיה)\n",
            "----------------------------------------\n",
            "Tav at index 96259 (Word: תשברו)\n",
            "Vav at index 96309 (Word: המול)\n",
            "Resh at index 96359 (Word: תורה)\n",
            "Hey at index 96409 (Word: יהוה)\n",
            "----------------------------------------\n",
            "Tav at index 97983 (Word: את)\n",
            "Vav at index 98033 (Word: בחור)\n",
            "Resh at index 98083 (Word: וירדף)\n",
            "Hey at index 98133 (Word: אחריהם)\n",
            "----------------------------------------\n",
            "Tav at index 98408 (Word: את)\n",
            "Vav at index 98458 (Word: ישועת)\n",
            "Resh at index 98508 (Word: לראתם)\n",
            "Hey at index 98558 (Word: מה)\n",
            "----------------------------------------\n",
            "Tav at index 118680 (Word: את)\n",
            "Vav at index 118730 (Word: תעשנו)\n",
            "Resh at index 118780 (Word: זרת)\n",
            "Hey at index 118830 (Word: פטדה)\n",
            "----------------------------------------\n",
            "Tav at index 128449 (Word: מצאת)\n",
            "Vav at index 128499 (Word: טובי)\n",
            "Resh at index 128549 (Word: אשר)\n",
            "Hey at index 128599 (Word: הנה)\n",
            "----------------------------------------\n",
            "Tav at index 137978 (Word: תכלת)\n",
            "Vav at index 138028 (Word: וזרת)\n",
            "Resh at index 138078 (Word: הטור)\n",
            "Hey at index 138128 (Word: והטור)\n",
            "----------------------------------------\n",
            "Tav at index 145613 (Word: קרנת)\n",
            "Vav at index 145663 (Word: הוסר)\n",
            "Resh at index 145713 (Word: וכפר)\n",
            "Hey at index 145763 (Word: יביאנה)\n",
            "----------------------------------------\n",
            "Tav at index 160399 (Word: ממארת)\n",
            "Vav at index 160449 (Word: או)\n",
            "Resh at index 160499 (Word: יראה)\n",
            "Hey at index 160549 (Word: וצוה)\n",
            "----------------------------------------\n",
            "Tav at index 160445 (Word: בפשתים)\n",
            "Vav at index 160495 (Word: ואם)\n",
            "Resh at index 160545 (Word: עור)\n",
            "Hey at index 160595 (Word: הכהן)\n",
            "----------------------------------------\n",
            "Tav at index 188332 (Word: אתו)\n",
            "Vav at index 188382 (Word: דגלו)\n",
            "Resh at index 188432 (Word: ישראל)\n",
            "Hey at index 188482 (Word: צוה)\n",
            "----------------------------------------\n",
            "Tav at index 192945 (Word: ישרתו)\n",
            "Vav at index 192995 (Word: המוט)\n",
            "Resh at index 193045 (Word: אשר)\n",
            "Hey at index 193095 (Word: המזבח)\n",
            "----------------------------------------\n",
            "Tav at index 214753 (Word: ציצת)\n",
            "Vav at index 214803 (Word: יהוה)\n",
            "Resh at index 214853 (Word: אחריהם)\n",
            "Hey at index 214903 (Word: יהוה)\n",
            "----------------------------------------\n",
            "Tav at index 251957 (Word: תנגפו)\n",
            "Vav at index 252007 (Word: ותזדו)\n",
            "Resh at index 252057 (Word: כאשר)\n",
            "Hey at index 252107 (Word: יהוה)\n",
            "----------------------------------------\n",
            "Tav at index 268243 (Word: את)\n",
            "Vav at index 268293 (Word: ידו)\n",
            "Resh at index 268343 (Word: לפרעה)\n",
            "Hey at index 268393 (Word: הציף)\n",
            "----------------------------------------\n",
            "Tav at index 293214 (Word: זיתים)\n",
            "Vav at index 293264 (Word: ולא)\n",
            "Resh at index 293314 (Word: בקרבך)\n",
            "Hey at index 293364 (Word: הוא)\n",
            "----------------------------------------\n",
            "Tav at index 296217 (Word: הכתובה)\n",
            "Vav at index 296267 (Word: והנכרי)\n",
            "Resh at index 296317 (Word: אשר)\n",
            "Hey at index 296367 (Word: בה)\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This was freshly coded to compare results"
      ],
      "metadata": {
        "id": "rg_d5SXtj2Fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import csv\n",
        "\n",
        "INPUT_FILE = \"torah_letter_index.csv\"\n",
        "OUTPUT_FILE = \"yhvh_els_50.csv\"\n",
        "\n",
        "TARGET = [\"י\", \"ה\", \"ו\", \"ה\"]\n",
        "SKIP = 50\n",
        "\n",
        "\n",
        "def load_letters():\n",
        "    letters = []\n",
        "    with open(INPUT_FILE, newline='', encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            letters.append(row[\"Letter\"])\n",
        "    return letters\n",
        "\n",
        "\n",
        "def find_els_matches(letters):\n",
        "    matches = []\n",
        "    total = len(letters)\n",
        "\n",
        "    for i in range(total - SKIP * 3):\n",
        "        seq = [\n",
        "            letters[i],\n",
        "            letters[i + SKIP],\n",
        "            letters[i + SKIP * 2],\n",
        "            letters[i + SKIP * 3]\n",
        "        ]\n",
        "\n",
        "        if seq == TARGET:\n",
        "            match_indices = [\n",
        "                i + 1,\n",
        "                i + SKIP + 1,\n",
        "                i + SKIP * 2 + 1,\n",
        "                i + SKIP * 3 + 1\n",
        "            ]\n",
        "\n",
        "            matches.append({\n",
        "                \"StartIndex\": i + 1,\n",
        "                \"Indices\": \",\".join(map(str, match_indices)),\n",
        "                \"Letters\": \"\".join(seq)\n",
        "            })\n",
        "\n",
        "    return matches\n",
        "\n",
        "\n",
        "def save_results(matches):\n",
        "    with open(OUTPUT_FILE, \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"StartIndex\", \"Indices\", \"Letters\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(matches)\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"Loading Torah letters...\")\n",
        "    letters = load_letters()\n",
        "\n",
        "    print(\"Searching for יהוה in skip = 50...\")\n",
        "    matches = find_els_matches(letters)\n",
        "\n",
        "    save_results(matches)\n",
        "\n",
        "    print(\"\\n===== RESULTS =====\")\n",
        "    print(\"Total letters scanned:\", len(letters))\n",
        "    print(\"ELS skip:\", SKIP)\n",
        "    print(\"Matches found:\", len(matches))\n",
        "    print(\"Saved to:\", OUTPUT_FILE)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xApCCOlyj6Yv",
        "outputId": "620d77b4-4fb2-460b-a859-836b795cb107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Torah letters...\n",
            "Searching for יהוה in skip = 50...\n",
            "\n",
            "===== RESULTS =====\n",
            "Total letters scanned: 304493\n",
            "ELS skip: 50\n",
            "Matches found: 25\n",
            "Saved to: yhvh_els_50.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fuzzy vs Strict search of a 50-window sequence"
      ],
      "metadata": {
        "id": "nIzIYRe9mcnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# yhwh_hybrid_windowed.py\n",
        "# Input: torah_letter_index.csv (Index,Letter,Hebrew Word)\n",
        "# Output: yhwh_comparison.csv (one row per 50-letter window with any matches)\n",
        "\n",
        "import csv\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "INPUT_LETTER_CSV = \"torah_letter_index.csv\"\n",
        "OUTPUT_CSV = \"yhwh_comparison.csv\"\n",
        "\n",
        "WINDOW_SIZE = 50        # letters per window (you asked for intervals of 50 characters)\n",
        "TARGET = \"יהוה\"         # tetragram to detect\n",
        "TARGET_LEN = len(TARGET)\n",
        "FUZZY_THRESHOLD = 0.80  # similarity threshold for fuzzy matches (0..1). Tune as needed.\n",
        "\n",
        "def similarity(a: str, b: str) -> float:\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def load_letter_index(path):\n",
        "    \"\"\"Load torah_letter_index.csv and return two parallel lists: indices[], letters[]\"\"\"\n",
        "    indices = []\n",
        "    letters = []\n",
        "    with open(path, newline='', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        # Expect columns: Index, Letter, Hebrew Word\n",
        "        for r in reader:\n",
        "            idx_raw = r.get(\"Index\") or r.get(\"index\")\n",
        "            letter = r.get(\"Letter\") or r.get(\"letter\")\n",
        "            if idx_raw is None or letter is None:\n",
        "                continue\n",
        "            try:\n",
        "                idx = int(idx_raw)\n",
        "            except:\n",
        "                # try to strip and convert\n",
        "                idx = int(str(idx_raw).strip())\n",
        "            letter = str(letter).strip()\n",
        "            if letter == \"\":\n",
        "                continue\n",
        "            indices.append(idx)\n",
        "            letters.append(letter)\n",
        "    return indices, letters\n",
        "\n",
        "def run_hybrid_scan(indices, letters,\n",
        "                    window_size=WINDOW_SIZE,\n",
        "                    target=TARGET,\n",
        "                    target_len=TARGET_LEN,\n",
        "                    fuzzy_thresh=FUZZY_THRESHOLD):\n",
        "    N = len(letters)\n",
        "    results = []\n",
        "    total_exact = 0\n",
        "    total_fuzzy = 0\n",
        "\n",
        "    # Precompute target letters as list\n",
        "    target_letters = list(target)\n",
        "\n",
        "    # iterate windows by starting position in letters array\n",
        "    for start in range(0, N - window_size + 1):\n",
        "        window_letters = letters[start:start + window_size]\n",
        "        window_indices = indices[start:start + window_size]\n",
        "        exact_positions = []\n",
        "        fuzzy_matches = []\n",
        "\n",
        "        # Exact contiguous search inside this window\n",
        "        # check every possible 4-letter start within the window\n",
        "        for j in range(0, window_size - target_len + 1):\n",
        "            seg = window_letters[j:j + target_len]\n",
        "            if seg == target_letters:\n",
        "                # map to the true Index of the first letter\n",
        "                pos_index = window_indices[j]\n",
        "                exact_positions.append(pos_index)\n",
        "\n",
        "        # Fuzzy search: slide a target_len window and compute similarity\n",
        "        for j in range(0, window_size - target_len + 1):\n",
        "            seg_letters = window_letters[j:j + target_len]\n",
        "            seg_text = \"\".join(seg_letters)\n",
        "            score = similarity(seg_text, target)\n",
        "            if score >= fuzzy_thresh:\n",
        "                pos_index = window_indices[j]\n",
        "                # store segment, score (rounded), start index\n",
        "                fuzzy_matches.append((pos_index, seg_text, round(score, 3)))\n",
        "\n",
        "        if exact_positions or fuzzy_matches:\n",
        "            results.append({\n",
        "                \"WindowStartIndex\": window_indices[0],\n",
        "                \"WindowEndIndex\": window_indices[-1],\n",
        "                \"ExactCount\": len(exact_positions),\n",
        "                \"ExactPositions\": \",\".join(map(str, exact_positions)) if exact_positions else \"\",\n",
        "                \"FuzzyCount\": len(fuzzy_matches),\n",
        "                # fuzzy details as 'Index:Segment:Score' separated by |\n",
        "                \"FuzzyDetails\": \"|\".join(f\"{pos}:{seg}:{score}\" for (pos, seg, score) in fuzzy_matches)\n",
        "            })\n",
        "            total_exact += len(exact_positions)\n",
        "            total_fuzzy += len(fuzzy_matches)\n",
        "\n",
        "    summary = {\n",
        "        \"LettersScanned\": N,\n",
        "        \"WindowsScanned\": max(0, N - window_size + 1),\n",
        "        \"TotalExactMatches\": total_exact,\n",
        "        \"TotalFuzzyMatches\": total_fuzzy\n",
        "    }\n",
        "    return results, summary\n",
        "\n",
        "def save_results(path, rows):\n",
        "    fieldnames = [\"WindowStartIndex\",\"WindowEndIndex\",\"ExactCount\",\"ExactPositions\",\"FuzzyCount\",\"FuzzyDetails\"]\n",
        "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for r in rows:\n",
        "            writer.writerow(r)\n",
        "\n",
        "def main():\n",
        "    print(\"Loading letters from:\", INPUT_LETTER_CSV)\n",
        "    indices, letters = load_letter_index(INPUT_LETTER_CSV)\n",
        "    if not letters:\n",
        "        print(\"No letters loaded. Check file and headers (Index,Letter,Hebrew Word).\")\n",
        "        return\n",
        "\n",
        "    print(f\"Total letters loaded: {len(letters)}\")\n",
        "    print(f\"Running hybrid scan: window={WINDOW_SIZE}, target='{TARGET}', fuzzy_thresh={FUZZY_THRESHOLD}\")\n",
        "\n",
        "    results, summary = run_hybrid_scan(indices, letters,\n",
        "                                      window_size=WINDOW_SIZE,\n",
        "                                      target=TARGET,\n",
        "                                      target_len=len(TARGET),\n",
        "                                      fuzzy_thresh=FUZZY_THRESHOLD)\n",
        "\n",
        "    save_results(OUTPUT_CSV, results)\n",
        "\n",
        "    print(\"\\n=== SCAN SUMMARY ===\")\n",
        "    print(\"Letters scanned:\", summary[\"LettersScanned\"])\n",
        "    print(\"Windows scanned:\", summary[\"WindowsScanned\"])\n",
        "    print(\"Total exact matches found:\", summary[\"TotalExactMatches\"])\n",
        "    print(\"Total fuzzy matches found:\", summary[\"TotalFuzzyMatches\"])\n",
        "    print(\"Results saved to:\", OUTPUT_CSV)\n",
        "    print(\"\\nSample output rows:\", min(5, len(results)))\n",
        "    for r in results[:5]:\n",
        "        print(r)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkTfHLUqmlEM",
        "outputId": "d2b89dd4-fda9-4abf-cd6c-f9f9ba997fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading letters from: torah_letter_index.csv\n",
            "Total letters loaded: 304493\n",
            "Running hybrid scan: window=50, target='יהוה', fuzzy_thresh=0.8\n",
            "\n",
            "=== SCAN SUMMARY ===\n",
            "Letters scanned: 304493\n",
            "Windows scanned: 304444\n",
            "Total exact matches found: 86386\n",
            "Total fuzzy matches found: 86386\n",
            "Results saved to: yhwh_comparison.csv\n",
            "\n",
            "Sample output rows: 5\n",
            "{'WindowStartIndex': 1803, 'WindowEndIndex': 1852, 'ExactCount': 1, 'ExactPositions': '1849', 'FuzzyCount': 1, 'FuzzyDetails': '1849:יהוה:1.0'}\n",
            "{'WindowStartIndex': 1804, 'WindowEndIndex': 1853, 'ExactCount': 1, 'ExactPositions': '1849', 'FuzzyCount': 1, 'FuzzyDetails': '1849:יהוה:1.0'}\n",
            "{'WindowStartIndex': 1805, 'WindowEndIndex': 1854, 'ExactCount': 1, 'ExactPositions': '1849', 'FuzzyCount': 1, 'FuzzyDetails': '1849:יהוה:1.0'}\n",
            "{'WindowStartIndex': 1806, 'WindowEndIndex': 1855, 'ExactCount': 1, 'ExactPositions': '1849', 'FuzzyCount': 1, 'FuzzyDetails': '1849:יהוה:1.0'}\n",
            "{'WindowStartIndex': 1807, 'WindowEndIndex': 1856, 'ExactCount': 1, 'ExactPositions': '1849', 'FuzzyCount': 1, 'FuzzyDetails': '1849:יהוה:1.0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More sequences"
      ],
      "metadata": {
        "id": "wv7CZwT1mgAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "yhwh_full_analysis.py\n",
        "\n",
        "Input:\n",
        " - torah_letter_index.csv  (columns: Index,Letter,Hebrew Word)\n",
        "\n",
        "Outputs:\n",
        " - exact_occurrences.csv\n",
        " - els_matches.csv\n",
        " - fuzzy_matches.csv\n",
        " - spacing_stats.csv\n",
        " - clusters.csv\n",
        " - summary.txt\n",
        "\n",
        "Configurable parameters are near the top of the file.\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "import math\n",
        "import sys\n",
        "from collections import Counter\n",
        "from difflib import SequenceMatcher\n",
        "from statistics import mean, median, pstdev\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "INPUT_LETTER_CSV = \"torah_letter_index.csv\"\n",
        "\n",
        "OUTPUT_EXACT = \"exact_occurrences.csv\"\n",
        "OUTPUT_ELS = \"els_matches.csv\"\n",
        "OUTPUT_FUZZY = \"fuzzy_matches.csv\"\n",
        "OUTPUT_SPACING = \"spacing_stats.csv\"\n",
        "OUTPUT_CLUSTERS = \"clusters.csv\"\n",
        "OUTPUT_SUMMARY = \"summary.txt\"\n",
        "\n",
        "TARGET = \"יהוה\"\n",
        "TARGET_LETTERS = list(TARGET)\n",
        "\n",
        "# ELS scanning: search skips s in 1..MAX_SKIP\n",
        "MAX_SKIP = 200      # default; increase to search larger skips (slower)\n",
        "\n",
        "# Fuzzy contiguous scanning parameters\n",
        "FUZZY_ENABLED = True\n",
        "FUZZY_THRESHOLD = 0.80    # similarity threshold (0..1)\n",
        "FUZZY_MIN_LEN = 3\n",
        "FUZZY_MAX_LEN = 6\n",
        "\n",
        "# Clustering gap threshold (letters). If consecutive occurrences are <= CLUSTER_GAP apart they are in same cluster\n",
        "CLUSTER_GAP = 200\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "def similarity(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def load_letters(path):\n",
        "    indices = []\n",
        "    letters = []\n",
        "    words = []\n",
        "    with open(path, newline='', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for r in reader:\n",
        "            idx_raw = r.get(\"Index\") or r.get(\"index\")\n",
        "            letter = r.get(\"Letter\") or r.get(\"letter\")\n",
        "            word = r.get(\"Hebrew Word\") or r.get(\"HebrewWord\") or r.get(\"word\")\n",
        "            if idx_raw is None or letter is None:\n",
        "                continue\n",
        "            try:\n",
        "                idx = int(str(idx_raw).strip())\n",
        "            except:\n",
        "                continue\n",
        "            letter = str(letter).strip()\n",
        "            indices.append(idx)\n",
        "            letters.append(letter)\n",
        "            words.append(word if word is not None else \"\")\n",
        "    return indices, letters, words\n",
        "\n",
        "def write_csv(path, header, rows):\n",
        "    with open(path, \"w\", newline='', encoding='utf-8') as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow(header)\n",
        "        for row in rows:\n",
        "            w.writerow(row)\n",
        "\n",
        "# ---------------- MAIN ANALYSIS ----------------\n",
        "def main():\n",
        "    print(\"Loading letters from:\", INPUT_LETTER_CSV)\n",
        "    indices, letters, words = load_letters(INPUT_LETTER_CSV)\n",
        "    N = len(letters)\n",
        "    if N == 0:\n",
        "        print(\"No letters found. Check input file.\")\n",
        "        return\n",
        "    print(\"Total letters:\", N)\n",
        "\n",
        "    # Build a mapping from index position in array -> global letter index\n",
        "    # indices[i] is the original global index of letters[i]\n",
        "\n",
        "    # 1) Exact contiguous detection (יהוה)\n",
        "    exact_positions = []   # store starting global index of each occurrence\n",
        "    target_len = len(TARGET_LETTERS)\n",
        "\n",
        "    for i in range(0, N - target_len + 1):\n",
        "        if letters[i:i + target_len] == TARGET_LETTERS:\n",
        "            exact_positions.append(indices[i])  # store global index of first letter\n",
        "\n",
        "    # Reduce to unique (they are unique by construction)\n",
        "    unique_exact_positions = exact_positions  # already unique by start index\n",
        "    total_exact = len(unique_exact_positions)\n",
        "    print(\"Exact contiguous occurrences found:\", total_exact)\n",
        "\n",
        "    # Write exact occurrences CSV\n",
        "    exact_rows = []\n",
        "    for pos in unique_exact_positions:\n",
        "        # find array index\n",
        "        arr_pos = None\n",
        "        # binary search since indices is increasing\n",
        "        lo = 0; hi = N-1\n",
        "        while lo <= hi:\n",
        "            mid = (lo + hi)//2\n",
        "            if indices[mid] == pos:\n",
        "                arr_pos = mid\n",
        "                break\n",
        "            elif indices[mid] < pos:\n",
        "                lo = mid + 1\n",
        "            else:\n",
        "                hi = mid - 1\n",
        "        word_at = words[arr_pos] if arr_pos is not None else \"\"\n",
        "        # also include the 4-letter sequence (should be יהוה)\n",
        "        seq_letters = \"\".join(letters[arr_pos:arr_pos+target_len]) if arr_pos is not None else \"\"\n",
        "        exact_rows.append([pos, arr_pos, seq_letters, word_at])\n",
        "    write_csv(OUTPUT_EXACT, [\"StartGlobalIndex\",\"ArrayIndex\",\"Letters\",\"HebrewWord\"], exact_rows)\n",
        "\n",
        "    # 2) Spacing statistics between consecutive exact occurrences\n",
        "    spacing_list = []\n",
        "    if total_exact >= 2:\n",
        "        # sort positions (they should already be sorted)\n",
        "        sorted_pos = sorted(unique_exact_positions)\n",
        "        for a, b in zip(sorted_pos, sorted_pos[1:]):\n",
        "            spacing_list.append(b - a)\n",
        "    # Compute histogram\n",
        "    spacing_counts = Counter(spacing_list)\n",
        "    spacing_rows = []\n",
        "    for span, cnt in sorted(spacing_counts.items()):\n",
        "        spacing_rows.append([span, cnt])\n",
        "    # include summary stats to spacing CSV later\n",
        "    write_csv(OUTPUT_SPACING, [\"GapBetweenOccurrences\",\"Count\"], spacing_rows)\n",
        "\n",
        "    # 3) Clustering consecutive occurrences by gap threshold (simple 1D clustering)\n",
        "    clusters = []\n",
        "    if total_exact > 0:\n",
        "        cur_cluster = [unique_exact_positions[0]]\n",
        "        for pos in unique_exact_positions[1:]:\n",
        "            if pos - cur_cluster[-1] <= CLUSTER_GAP:\n",
        "                cur_cluster.append(pos)\n",
        "            else:\n",
        "                clusters.append(cur_cluster)\n",
        "                cur_cluster = [pos]\n",
        "        clusters.append(cur_cluster)\n",
        "    cluster_rows = []\n",
        "    for c in clusters:\n",
        "        cluster_rows.append([c[0], c[-1], len(c), \",\".join(map(str,c))])\n",
        "    write_csv(OUTPUT_CLUSTERS, [\"ClusterStart\",\"ClusterEnd\",\"Count\",\"PositionsCSV\"], cluster_rows)\n",
        "\n",
        "    # 4) ELS search for skips 1..MAX_SKIP\n",
        "    print(\"Running ELS search for skips 1..\", MAX_SKIP)\n",
        "    # We'll collect matches as: skip, start_global_index (pos1), pos2, pos3, pos4, span, letters_concat\n",
        "    els_matches = []\n",
        "    max_skip = MAX_SKIP\n",
        "    # For performance, we'll access letters by index variables\n",
        "    for s in range(1, max_skip + 1):\n",
        "        # last starting index in array: N - 3*s - 1\n",
        "        last_start = N - 3 * s\n",
        "        if last_start <= 0:\n",
        "            break\n",
        "        # optimize by pulling the target letters\n",
        "        t0, t1, t2, t3 = TARGET_LETTERS\n",
        "        # iterate start i\n",
        "        for i in range(0, last_start):\n",
        "            if letters[i] != t0:\n",
        "                continue\n",
        "            # quickly check the rest\n",
        "            if letters[i + s] == t1 and letters[i + 2 * s] == t2 and letters[i + 3 * s] == t3:\n",
        "                pos1 = indices[i]\n",
        "                pos2 = indices[i + s]\n",
        "                pos3 = indices[i + 2 * s]\n",
        "                pos4 = indices[i + 3 * s]\n",
        "                span = pos4 - pos1 + 1\n",
        "                letters_concat = letters[i] + letters[i + s] + letters[i + 2 * s] + letters[i + 3 * s]\n",
        "                els_matches.append([s, pos1, pos2, pos3, pos4, span, letters_concat])\n",
        "        # optional progress print for large runs\n",
        "        if s % 50 == 0:\n",
        "            print(\"  scanned skip\", s)\n",
        "    write_csv(OUTPUT_ELS, [\"Skip\",\"Pos1\",\"Pos2\",\"Pos3\",\"Pos4\",\"Span\",\"LettersConcat\"], els_matches)\n",
        "    print(\"ELS matches found:\", len(els_matches))\n",
        "\n",
        "    # 5) Fuzzy contiguous matches (optional)\n",
        "    fuzzy_rows = []\n",
        "    total_fuzzy = 0\n",
        "    if FUZZY_ENABLED:\n",
        "        print(\"Running fuzzy contiguous scan (len\", FUZZY_MIN_LEN, \"-\", FUZZY_MAX_LEN, \"), threshold\", FUZZY_THRESHOLD)\n",
        "        for L in range(FUZZY_MIN_LEN, FUZZY_MAX_LEN + 1):\n",
        "            if N < L:\n",
        "                continue\n",
        "            for i in range(0, N - L + 1):\n",
        "                seg = \"\".join(letters[i:i+L])\n",
        "                score = similarity(seg, TARGET)\n",
        "                if score >= FUZZY_THRESHOLD:\n",
        "                    pos = indices[i]\n",
        "                    span = indices[i+L-1] - pos + 1\n",
        "                    fuzzy_rows.append([pos, i, seg, L, round(score,3), span])\n",
        "                    total_fuzzy += 1\n",
        "            # small progress\n",
        "            # print(\"  finished length\", L)\n",
        "    write_csv(OUTPUT_FUZZY, [\"StartGlobalIndex\",\"ArrayIndex\",\"Segment\",\"SegmentLen\",\"Score\",\"Span\"], fuzzy_rows)\n",
        "    print(\"Fuzzy matches found:\", total_fuzzy)\n",
        "\n",
        "    # 6) Appearance window sizes: for exact contiguous -> span = 4 (always); for ELS we computed span.\n",
        "    # aggregate span distribution across all matches (exact + els + fuzzy)\n",
        "    span_counter = Counter()\n",
        "    # exact contiguous spans (should be 4)\n",
        "    for pos in unique_exact_positions:\n",
        "        span_counter[4] += 1\n",
        "    for row in els_matches:\n",
        "        span_counter[row[5]] += 1\n",
        "    for row in fuzzy_rows:\n",
        "        span_counter[row[5]] += 1\n",
        "    span_rows = sorted([(span, cnt) for span, cnt in span_counter.items()], key=lambda x: x[0])\n",
        "    # write appearance window sizes to a CSV (reuse spacing CSV file? create clusters file)\n",
        "    # we'll append to spacing_stats.csv for convenience (or write separate)\n",
        "    # create a combined spacing/appearance CSV\n",
        "    appearance_file = \"appearance_window_sizes.csv\"\n",
        "    write_csv(appearance_file, [\"SpanChars\",\"Count\"], span_rows)\n",
        "\n",
        "    # 7) Summary statistics and spacing basic metrics\n",
        "    unique_sorted = sorted(unique_exact_positions)\n",
        "    gaps = [b - a for a, b in zip(unique_sorted, unique_sorted[1:])] if len(unique_sorted) >= 2 else []\n",
        "    summary_lines = []\n",
        "    summary_lines.append(f\"Total letters: {N}\")\n",
        "    summary_lines.append(f\"Exact contiguous יהוה occurrences: {total_exact}\")\n",
        "    summary_lines.append(f\"ELS matches total: {len(els_matches)} (skips scanned 1..{MAX_SKIP})\")\n",
        "    summary_lines.append(f\"Fuzzy matches total: {total_fuzzy} (threshold {FUZZY_THRESHOLD})\")\n",
        "    summary_lines.append(\"\")\n",
        "    if gaps:\n",
        "        summary_lines.append(\"Spacing between consecutive exact occurrences (basic):\")\n",
        "        summary_lines.append(f\"  min gap: {min(gaps)}\")\n",
        "        summary_lines.append(f\"  mean gap: {mean(gaps):.2f}\")\n",
        "        summary_lines.append(f\"  median gap: {median(gaps)}\")\n",
        "        summary_lines.append(f\"  stddev gap: {pstdev(gaps):.2f}\")\n",
        "        # produce top 20 most common gaps\n",
        "        gap_counts = Counter(gaps)\n",
        "        most_common_gaps = gap_counts.most_common(20)\n",
        "        summary_lines.append(\"  most common gaps (gap:count): \" + \", \".join(f\"{g}:{c}\" for g,c in most_common_gaps))\n",
        "    else:\n",
        "        summary_lines.append(\"Not enough exact occurrences to compute gaps.\")\n",
        "\n",
        "    # clusters summary\n",
        "    summary_lines.append(\"\")\n",
        "    summary_lines.append(f\"Clusters found (gap threshold {CLUSTER_GAP}): {len(clusters)}\")\n",
        "    cluster_sizes = [len(c) for c in clusters]\n",
        "    if cluster_sizes:\n",
        "        summary_lines.append(f\"  largest cluster size: {max(cluster_sizes)}\")\n",
        "        summary_lines.append(f\"  mean cluster size: {mean(cluster_sizes):.2f}\")\n",
        "\n",
        "    # span distribution summary\n",
        "    summary_lines.append(\"\")\n",
        "    summary_lines.append(\"Appearance window sizes (span chars) summary (span:count):\")\n",
        "    for span, cnt in span_rows[:40]:\n",
        "        summary_lines.append(f\"  {span}: {cnt}\")\n",
        "\n",
        "    # write summary to file\n",
        "    with open(OUTPUT_SUMMARY, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(summary_lines))\n",
        "\n",
        "    # Print short summary to console\n",
        "    print(\"\\n=== ANALYSIS SUMMARY ===\")\n",
        "    for line in summary_lines[:12]:\n",
        "        print(line)\n",
        "    print(\"... (full summary written to\", OUTPUT_SUMMARY, \")\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Czh3plMpPNi",
        "outputId": "af00ffa2-5d8c-4bf9-80f4-074b54b9e9b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading letters from: torah_letter_index.csv\n",
            "Total letters: 304493\n",
            "Exact contiguous occurrences found: 1838\n",
            "Running ELS search for skips 1.. 200\n",
            "  scanned skip 50\n",
            "  scanned skip 100\n",
            "  scanned skip 150\n",
            "  scanned skip 200\n",
            "ELS matches found: 7142\n",
            "Running fuzzy contiguous scan (len 3 - 6 ), threshold 0.8\n",
            "Fuzzy matches found: 15867\n",
            "\n",
            "=== ANALYSIS SUMMARY ===\n",
            "Total letters: 304493\n",
            "Exact contiguous יהוה occurrences: 1838\n",
            "ELS matches total: 7142 (skips scanned 1..200)\n",
            "Fuzzy matches total: 15867 (threshold 0.8)\n",
            "\n",
            "Spacing between consecutive exact occurrences (basic):\n",
            "  min gap: 4\n",
            "  mean gap: 164.70\n",
            "  median gap: 76\n",
            "  stddev gap: 532.41\n",
            "  most common gaps (gap:count): 28:25, 38:22, 51:22, 19:20, 14:20, 33:20, 39:19, 36:19, 31:19, 24:19, 42:18, 44:18, 62:18, 9:18, 37:18, 21:18, 48:18, 22:17, 50:16, 35:16\n",
            "\n",
            "... (full summary written to summary.txt )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save file sessions"
      ],
      "metadata": {
        "id": "fL9OIIsVj5GM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Create a destination folder in Drive\n",
        "destination_folder = '/content/drive/MyDrive/Colab_Project_Files/LXX_DSS_Torahcode'\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "\n",
        "# 3. Run the sync command\n",
        "# Flags explained:\n",
        "# -a: archive mode (keeps permissions/dates)\n",
        "# -v: verbose (shows progress)\n",
        "# --exclude 'drive': CRITICAL. Prevents trying to copy Drive into itself.\n",
        "# --exclude 'sample_data': Skips the default Colab sample files.\n",
        "!rsync -av --exclude='drive' --exclude='.config' --exclude='sample_data' /content/ \"$destination_folder\"\n",
        "\n",
        "print(f\"Files synced to {destination_folder}\")"
      ],
      "metadata": {
        "id": "gQXexOh5qR_o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}